
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.text-justify {
    text-align: justify;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: rgb(73, 73, 73);
    text-align: center;
    margin-top: 15px;
    margin-bottom: 12px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 18px;
  width: 90px;
  font-weight: 400;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}



.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}
.controls {
  margin-bottom: 10px;
  margin-top: 20px;
}
.left-controls {
  display: inline-block;
  vertical-align: top;
  width: 80%;
}
.right-controls {
  display: inline-block;
  vertical-align: top;
  width: 19%;
  text-align: right;
}

.render_window {
    display: inline-block;
    vertical-align: middle;
    box-shadow: 0px 0px 0px black;
    margin-right: 20px;
    margin-bottom: 20px;
    width: calc(33% - 20px);
}

</style>


<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>Neural Fields as Learnable Kernels for 3D Reconstruction</title>
    <meta property="og:description" content="Neural Fields as Learnable Kernels for 3D Reconstruction"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>Neural Fields as Learnable Kernels for 3D Reconstruction</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-4 text-center"><a href="https://cims.nyu.edu/~francisw/">Francis Williams</a><sup>1,2*</sup></div>
            <div class="col-4 text-center"><a href="https://zgojcic.github.io/">Zan Gojcic</a><sup>1,3*</sup></div>
            <div class="col-4 text-center"><a href="https://www.samehkhamis.com/">Sameh Khamis</a><sup>1</sup></div>
            <div class="col-4 text-center"><a href="https://cims.nyu.edu/gcl/denis.html">Denis Zorin</a><sup>2</sup></div>
            <div class="col-3 text-center"><a href="https://cims.nyu.edu/~bruna/">Joan Bruna</a><sup>2</sup></div>
            <div class="col-3 text-center"><a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a><sup>1,4,5</sup></div>
            <div class="col-3 text-center"><a href="https://orlitany.github.io/">Or Litany</a><sup>1</sup></div>                    
        </div>

        <div class="affil-row">
            <div class="col-5 text-center"><sup>1</sup>NVIDIA</a></div>
            <div class="col-5 text-center"><sup>2</sup>New York University</div>
            <div class="col-5 text-center"><sup>3</sup>ETH Zurich</div>
            <div class="col-5 text-center"><sup>4</sup>University of Toronto</div>
            <div class="col-5 text-center"><sup>5</sup>Vector Institute</div>
        </div>
        <!-- <div class="affil-row">
            <div class="venue text-center"><b></b></div>
        </div> -->

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="assets/Neural_Kernel_Fields.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="supp-btn" href="assets/bib.txt">
                <span class="material-icons"> description </span> 
                  BibTeX
            </a>
        </div></div>
    </div>

    <section id="teaser">
            <figure style="width: 100%;">
                <a href="assets/teaser.png">
                    <img width="100%" src="assets/teaser_low_res.png">
                </a>
                <p class="caption">
                    Trained only on some categories of synthetic objects, Neural Kernel Fields (NKF) can 
                    reconstruct objects in and out of the training distribution, 
                    and can even directly generalize to scanned scenes.
                </p>
            </figure>
    </section>

    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
        <p>
            We present Neural Kernel Fields: a novel method for
            reconstructing implicit 3D shapes based on a learned kernel
            ridge regression. Our technique achieves state-of-the-art
            results when reconstructing 3D objects and large scenes from
            sparse oriented points, and can reconstruct shape categories
            outside the training set with almost no drop in accuracy.
            The core insight of our approach is that kernel methods
            are extremely effective for reconstructing shapes when the
            chosen kernel has an appropriate inductive bias. We thus
            factor the problem of shape reconstruction into two parts: (1)
            a backbone neural network which learns kernel parameters
            from data, and (2) a kernel ridge regression that fits the
            input points on-the-fly by solving a simple positive definite
            linear system using the learned kernel. As a result of this
            factorization, our reconstruction gains the benefits of data-
            driven methods under sparse point density while maintaining
            interpolatory behavior, which converges to the ground truth
            shape as input sampling density increases. Our experiments
            demonstrate a strong generalization capability to objects
            outside the train-set category and scanned scenes.
        </p>
    </section>

    <section id="method">
    <h2>Method</h2>
    <hr>
        <figure style="width: 100%;">
            <a href="assets/network_architecture.jpeg">
                <img width="100%" src="assets/network_architecture_low_res.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
                Our method works in two stages: (1) <b> prediction </b> (Top row) where we predict an implicit function from an input point
                cloud, and (2) <b>evaluation</b> (Bottom row) where we evaluate the implicit function. Our predicted implicit consists of a feature
                function \(\phi\), which lifts points in the volume to features in \(R^d\), and a set of coefficients \(\boldsymbol{\alpha}\), which are used to encode the function
                as a linear combination of basis functions centered at the input points.
            </p>
        </figure>
    </section>


    <section id="Results">
        <h2>Results</h2>
    <hr>
    <h3>Single object reconstruction on ShapeNet dataset</h3>

    <section id="shapenet">
        <div class="controls">
            <div class="left-controls", style="font-size:100%">
                Show:
                <input type="checkbox" id="shapenet_surface_mesh"><label for="shapenet_surface_mesh">surface-mesh</label>
                <input type="checkbox" id="shapenet_points"><label for="shapenet_points">points</label>
            </div>
            <div class="right-controls", style="font-size:100%">
                <button>Load new shape</button>
            </div>
        </div> 
        <div class="render_container">
            <div data-size="400" class="render_window"></div>
            <div data-size="400" class="render_window"></div>
            <div data-size="400" class="render_window"></div>
        </div>
        <div class="col-3 text-center", style="font-weight: bold">C-OccNet</div>
        <div class="col-3 text-center", style="font-weight: bold">Ours</div>
        <div class="col-3 text-center", style="font-weight: bold; margin-bottom: 2cm;">Ground truth</div>

    </section>
    <p class="caption", style="margin-top: 1cm; margin-bottom: 1cm">
        In-category single shape reconstruction. <b> Visualizations are interactive </b>. To load a new random shape press the button in the top right corner.
    </p>
    <hr>
    <h3>Generalization to scanned real world scenes</h3>
    <section id="videos">
        <figure style="width: 50%; float: left">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/scannet_1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="width: 50%; float: right">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/scannet_2.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <p class="caption" style="margin-bottom: 1px">
            Qualitative side-by-side comparison of our method (<span style="color: rgb(117, 117, 117)"><b>gray</b></span>)
            to the baselines (<span style="color: rgb(164, 116, 187)"><b>violet</b></span>) on scene level reconstruction.
            Note that our method is trained only on synthetic shapes from ShapeNet dataset.
        </p>
    </section>

    <hr>
    <h3>Out of category generalization - model trained on half categories</h3>
    <figure>
        <a href="assets/shapenet_generalization.png">
            <img width="100%" src="assets/shapenet_generalization_low_res.png">
        </a>
        <div class="col-4 text-center">OccNet</div>
        <div class="col-4 text-center">C-OccNet</div>
        <div class="col-4 text-center">Ours</div>
        <div class="col-4 text-center", style="margin-bottom: 1cm">Ground truth</div>
        <p class="caption", style="margin-top: 1cm; margin-bottom: 1cm">
            Out-of-category generalization. Models trained on categories <i>airplane, lamp, display, rifle, chair, and cabinet</i> 
            are used to reconstruct the shapes from other seven categories of ShapeNet dataset.
        </p>
    </figure>

    <hr>
    <h3>More extreme generalization - model trained only on chairs</h3>
    <figure>
        <a href="assets/shapenet_extreme_generalization.png">
            <img width="100%" src="assets/shapenet_extreme_generalization_low_res.png">
        </a>
        <div class="col-3 text-center">Ours trained on chairs only</div>
        <div class="col-3 text-center">Ours trained on all categories</div>
        <div class="col-3 text-center", style="padding-left:7px; margin-bottom: 1cm">Ground truth</div>
        <p class="caption", style="margin-top: 1cm; margin-bottom: 1cm">
            Our model trained only on chairs (left) can seamlessly generalize to other 12 
            ShapeNet categories, achieving only slightly worse performance than the model trained on all categories (middle). 
        </p>
    </figure>

    
    </section>
    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>
            @article{williams2021nkf,
            author = {Francis williams and Zan Gojcic and Sameh Khamis and 
                      Denis Zorin and Joan Bruna and Sanja Fidler and Or Litany}
            title = "{Neural Fields as Learnable Kernels for 3D Reconstruction}",
            journal = {INSERT ARXIV ID},
            year = {2021}}
        </code></pre>
    </section>

    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <figure style="width: 100%;">
            <a href="assets/Neural_Kernel_Fields.pdf">
                <img width="100%" src="assets/paper.png">
            </a>
        </figure>
        <hr>
    </section>


<script type="module">
    import * as THREE from "https://unpkg.com/three@0.127.0/build/three.module.js";
    import {OrbitControls} from "https://unpkg.com/three@0.127.0/examples/jsm/controls/OrbitControls.js";
    import {OBJLoader} from "https://unpkg.com/three@0.127.0/examples/jsm/loaders/OBJLoader.js";

    // Render the predictions
    function random_choice(arr, n) {
        var index_set = {};
        var choice = [];
        while (choice.length < n) {
            var idx = Math.floor(Math.random() * arr.length);
            if (index_set[idx] !== undefined) {
                continue;
            }
            index_set[idx] = 0;
            choice.push(idx);
        }

        return choice.map(x => arr[x]);
    }

    function progress_bar() {
        var el = document.createElement("div");
        el.classList.add("progress");

        return {
            domElement: el,
            update: function (percent) {
                percent = Math.min(1, Math.max(0, percent));
                el.style.display = "block";
                el.style.width = Math.round(percent * 100) + "%";
            },
            hide: function () {
                el.style.display = "none";
            }
        };
    }

    function reset_checkboxes(checkboxes) {
        Array.prototype.forEach.call(checkboxes, function (c) {
            c.checked = false;
        });
        checkboxes[0].checked = true;
        checkboxes[1].checked = true;
    }

    function show_object(el, prefix, N) {
        const scene = new THREE.Scene();
        const renderer = new THREE.WebGLRenderer();
        const camera = new THREE.PerspectiveCamera(75, 1, 0.1, 1000);
        const controls = new OrbitControls(camera, renderer.domElement);

        camera.position.set(0.55, 0.55, 0.55);
        controls.target.set(0, 0, 0);
        controls.autoRotate = true;
        controls.autoRotateSpeed = 4;
        scene.background = new THREE.Color("white");
        var size = el.dataset.size;
        renderer.setSize(size, size);
        var progress = progress_bar();
        el.appendChild(progress.domElement);
        el.appendChild(renderer.domElement);

        const spotLight = new THREE.SpotLight( 0x909090 );
        spotLight.position.set( -100, -1000, -100 );

        spotLight.castShadow = true;
        scene.add( spotLight );

        const spotLight_2 = new THREE.SpotLight( 0x909090 );
        spotLight_2.position.set(100, 1000, 100 );

        spotLight_2.castShadow = true;
        scene.add( spotLight_2 );

        const spotLight_3= new THREE.SpotLight( 0x909090 );
        spotLight_3.position.set(0, 0, 100000 );

        spotLight_3.castShadow = true;
        scene.add( spotLight_3);

        const spotLight_4= new THREE.SpotLight( 0x909090 );
        spotLight_4.position.set(0, 0, -100000 );

        spotLight_4.castShadow = true;
        scene.add( spotLight_4);

        const colors = [
            0xf4f4f4,
            0xbc96dc,
        ];

        var previous_canvas_size = size;
        function animate() {
            requestAnimationFrame(animate);
            if (el.offsetWidth != previous_canvas_size) {
                previous_canvas_size = el.offsetWidth;
                renderer.domElement.style.width = previous_canvas_size + "px";
                renderer.domElement.style.height = previous_canvas_size + "px";
            }

            controls.update();
            renderer.render(scene, camera);
        }

        const loader = new OBJLoader();
        var meshes = [];
        var progresses = [];
        var loaded = 0;
        function load_part(part_idx) {
            progresses[part_idx] = 0;
            loader.load(
                prefix + "/part_00" + i + ".obj",
                function (object) {
                    var g = object.children[0].geometry;
                    var m = new THREE.MeshLambertMaterial({color: colors[part_idx]});
                    m.side = THREE.DoubleSide;
                    //g.computeVertexNormals();
                    var mesh = new THREE.Mesh(g, m);
                    meshes[part_idx] = mesh;

                    scene.add(mesh);

                    loaded++;
                    if (loaded == N) {
                        progress.hide();
                    }
                },
                function (event) {
                    progresses[part_idx] = event.loaded / event.total;
                    var total_progress = 0;
                    for (var i=0; i<progresses.length; i++) {
                        total_progress += progresses[i] / progresses.length;
                    }
                    progress.update(total_progress);
                }
            )
        }
        for (var i=0; i<N; i++) {
            load_part(i);
        }
        animate();

        return {
            meshes: meshes,
            show: function (indices) {
                for (var i=0; i<N; i++) {
                    //meshes[i].material.opacity = 0.0;
                    meshes[i].visible = false;
                }
                for (var i=0; i<indices.length; i++) {
                    //meshes[indices[i]].material.opacity = 1;
                    meshes[indices[i]].visible = true;
                }
            },
            show_all: function () {
                for (var i=0; i<N; i++) {
                    //meshes[i].material.opacity = 1;
                    meshes[i].visible = true;
                }
            },
            set_size: function(width, height) {
                renderer.setSize(width, height);
            }
        };
    }

    
    function show_group(elements, objects, N) {
        var controls = [];
        for (var i=0; i<elements.length; i++) {
            if (i==0) {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/conv_occnet/' + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            } else if (i==1) {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/ours/' + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            } else {
                var scene_name = 'https://raw.githubusercontent.com/nv-tlabs/nkf/main/assets/models/shapenet_reconstruction/ground_truth/'  + objects[0]
                controls.push(show_object(elements[i], scene_name, N));
            }
        }

        return {
            controls: controls,
            show: function (indices) {
                for (var i=0; i<controls.length; i++) {
                    controls[i].show(indices);
                }
            },
            show_all: function () {
                for (var i=0; i<controls.length; i++) {
                    controls[i].show_all();
                }
            }
        };
    }

    var shapenet = [
    "40",
    "1320",
    "1560",
    "1680",
    "2000",
    "2880",
    "3120",
    "3700",
    "3780",
    "4260",
    "4960",
    "5020",
    "5440",
    "6060",
    "6960",
    "7740",
    "8620",
    ];

    var shapenet_control = show_group(
        document.getElementById("shapenet").getElementsByClassName("render_window"),
        [shapenet[12], shapenet[1], shapenet[2]],
        2
    );
    var shapenet_checkboxes = document.querySelectorAll("#shapenet .controls input");
    reset_checkboxes(shapenet_checkboxes);
    document.querySelector("#shapenet .controls").addEventListener(
        "change",
        function (ev) {

            var ids = new Set();
            var part_ids = [0, 1];
            for (var i=0; i<shapenet_checkboxes.length; i++) {
                if (shapenet_checkboxes[i].checked) {
                    ids.add(part_ids[i]);
                }
            }

            shapenet_control.show(Array.from(ids));
        }
    );
    document.querySelector("#shapenet .controls button").addEventListener(
        "click",
        function (ev) {
            reset_checkboxes(shapenet_checkboxes);
            var new_shapenet = random_choice(shapenet, 1);
            var render_windows = document.getElementById("shapenet").getElementsByClassName("render_window");
            Array.prototype.forEach.call(render_windows, function (r) {r.innerHTML = "";});
            shapenet_control = show_group(
                render_windows,
                new_shapenet,
                2
            );
        }
    );



</script>
</body>
</html>